{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed8187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "nest_asyncio.apply()\n",
    "\n",
    "url = \"https://sa.iherb.com/c/tea?sr=14\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b5d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6977c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://sa.iherb.com/c/tea?sr=14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b589ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.FirefoxOptions()\n",
    "#options.add_argument(\"--headless\")  # Run the browser in headless mode\n",
    "options.add_argument(\"--window-size=1920,1080\")  # Set the window size\n",
    "driver = webdriver.Firefox(options=options)\n",
    "drv = webdriver.Firefox(options=options)\n",
    "# Open the webpage\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef0de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global n\n",
    "    n = 0\n",
    "    for p in range(1,15):\n",
    "        \n",
    "        urln = url.split('?p=')[0]+f'?p={p}'\n",
    "        \n",
    "        print(\"im done\")\n",
    "        \n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        \n",
    "\n",
    "\n",
    "        products=soup.find_all(\"div\",{'class':'product-cell-container'})\n",
    "\n",
    "\n",
    "        for p in products:\n",
    "            try:\n",
    "                old_price = p.find(\"span\", {\"class\":\"price-olp\"}).text.strip()\n",
    "\n",
    "            except:\n",
    "                old_price= None\n",
    "\n",
    "\n",
    "            if old_price:\n",
    "\n",
    "                print(old_price)\n",
    "\n",
    "\n",
    "                title =p.find(\"div\",{'class':'product-title'}).text.strip()\n",
    "                print(title)\n",
    "\n",
    "                try:\n",
    "                    product_url = p.find(\"a\",{'class':'product-link'}).get('href')\n",
    "                except:\n",
    "                    product_url = None\n",
    "                print(product_url)\n",
    "                \n",
    "                try:\n",
    "                    discount_rate = p.find(\"span\", {\"class\":\"percentage-off\"}).text.strip()\n",
    "                except:\n",
    "                    discount_rate = None\n",
    "                print(discount_rate)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    new_price = p.find(\"span\", {\"class\":\"price\"}).text.strip()\n",
    "                except:\n",
    "                    new_price=None\n",
    "                print(new_price)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "                drv.get(product_url)\n",
    "                html = drv.page_source\n",
    "                soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "                image_url = []\n",
    "                try:\n",
    "                    images = soup.find(\"div\",{'class':'thumbnail-container'})\n",
    "                    images = images.find_all('div',{'class':'thumbnail-item'})\n",
    "                    for image in images:\n",
    "                        #image = image.find(\"img\").get('src')\n",
    "                        image_url.append(image)\n",
    "                except:\n",
    "                    image_url = None\n",
    "                print(image_url)\n",
    "                \n",
    "                try:\n",
    "                    desc = soup.find(\"ul\", {\"id\":\"product-specs-list\"}).text.strip()\n",
    "                except:\n",
    "                    decs = None\n",
    "                print(desc)\n",
    "\n",
    "\n",
    "                n+=1\n",
    "                \n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28a7696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f0ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b32e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_url():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(url, headers=headers)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3779c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n",
      "im done\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "nest_asyncio.apply()\n",
    "url = \"https://sa.iherb.com/c/tea?sr=14\"\n",
    "async def main():\n",
    "    global n\n",
    "    n = 0\n",
    "    for p in range(1,15):\n",
    "        \"\"\"async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(url.split('?p=')[0]+f'?p={p}', headers=headers) \n",
    "            \"\"\"\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                async with httpx.AsyncClient() as client:\n",
    "                    response = await client.get(url.split('?p=')[0]+f'?p={p}', headers=headers, timeout=30)\n",
    "                break  # If successful, exit the loop\n",
    "            except httpx.ConnectTimeout:\n",
    "                print(f\"Attempt {attempt + 1} failed. Retrying...\")\n",
    "        print(\"im done\")\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        products=soup.find_all(\"div\",{'class':'product-cell-container'})\n",
    "\n",
    "\n",
    "        for p in products:\n",
    "            try:\n",
    "                old_price = p.find(\"span\", {\"class\":\"price-olp\"}).text.strip()\n",
    "\n",
    "            except:\n",
    "                old_price= None\n",
    "\n",
    "\n",
    "            if old_price:\n",
    "\n",
    "                print(old_price)\n",
    "\n",
    "\n",
    "                title =p.find(\"div\",{'class':'product-title'}).text.strip()\n",
    "                print(title)\n",
    "\n",
    "                try:\n",
    "                    product_url = p.find(\"a\",{'class':'product-link'}).get('href')\n",
    "                except:\n",
    "                    product_url = None\n",
    "                print(product_url)\n",
    "                \n",
    "                try:\n",
    "                    discount_rate = p.find(\"span\", {\"class\":\"percentage-off\"}).text.strip()\n",
    "                except:\n",
    "                    discount_rate = None\n",
    "                print(discount_rate)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    new_price = p.find(\"span\", {\"class\":\"price\"}).text.strip()\n",
    "                except:\n",
    "                    new_price=None\n",
    "                print(new_price)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                max_retries = 3\n",
    "                for attempt in range(max_retries):\n",
    "                    try:\n",
    "                        async with httpx.AsyncClient() as client:\n",
    "                            response = await client.get(product_url, headers=headers, timeout=20)\n",
    "                        break  # If successful, exit the loop\n",
    "                    except httpx.ConnectTimeout:\n",
    "                        print(f\"Attempt {attempt + 1} failed. Retrying...\")\n",
    "                        \n",
    "                html = response.text\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                image_url = []\n",
    "                try:\n",
    "                    images = soup.find(\"div\",{'class':'thumbnail-container'})\n",
    "                    images = images.find_all('div',{'class':'thumbnail-item'})\n",
    "                    for image in images:\n",
    "                        #image = image.find(\"img\").get('src')\n",
    "                        image_url.append(image)\n",
    "                except:\n",
    "                    image_url = None\n",
    "                print(image_url)\n",
    "                \n",
    "                try:\n",
    "                    desc = soup.find(\"ul\", {\"id\":\"product-specs-list\"}).text.strip()\n",
    "                except:\n",
    "                    decs = None\n",
    "                print(desc)\n",
    "\n",
    "\n",
    "                n+=1\n",
    "                \n",
    "                print()\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0cc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd75627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ccd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
